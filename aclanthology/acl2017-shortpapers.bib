@Book{Short:2017,
  editor    = {Regina Barzilay  and  Min-Yen Kan},
  title     = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {http://aclweb.org/anthology/P17-2}
}

@InProceedings{cheng-miyao:2017:Short,
  author    = {Cheng, Fei  and  Miyao, Yusuke},
  title     = {Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1--6},
  abstract  = {Temporal relation classification is becoming an active research field. Lots of
	methods have been proposed, while most of them focus on extracting features
	from external resources. Less attention has been paid to a significant advance
	in a closely related task: relation extraction. In this work, we borrow a
	state-of-the-art method in relation extraction by adopting bidirectional long
	short-term memory (Bi-LSTM) along dependency paths (DP). We make a “common
	root” assumption to extend DP representations of cross-sentence links. In the
	final comparison to two state-of-the-art systems on TimeBank-Dense, our model
	achieves comparable performance, without using external knowledge, as well as
	manually annotated attributes of entities (class, tense, polarity, etc.).},
  url       = {http://aclweb.org/anthology/P17-2001}
}

@InProceedings{song-EtAl:2017:Short,
  author    = {Song, Linfeng  and  Peng, Xiaochang  and  Zhang, Yue  and  Wang, Zhiguo  and  Gildea, Daniel},
  title     = {AMR-to-text Generation with Synchronous Node Replacement Grammar},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {7--13},
  abstract  = {This paper addresses the task of AMR-to-text generation by leveraging
	synchronous
	node replacement grammar. During training, graph-to-string rules are learned
	using a heuristic extraction algorithm. At test time, a graph transducer is
	applied to collapse input AMRs and generate output sentences. Evaluated on a
	standard benchmark, our method gives the state-of-the-art result.},
  url       = {http://aclweb.org/anthology/P17-2002}
}

@InProceedings{moosavi-strube:2017:Short,
  author    = {Moosavi, Nafise Sadat  and  Strube, Michael},
  title     = {Lexical Features in Coreference Resolution: To be Used With Caution},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {14--19},
  abstract  = {Lexical features are a major source of information in state-of-the-art
	coreference resolvers. Lexical features implicitly model some of the linguistic
	phenomena at a fine granularity level. They are especially useful for
	representing the context of mentions. In this paper we investigate a drawback
	of using many lexical features in state-of-the-art coreference resolvers. We
	show that if coreference resolvers mainly rely on lexical features, they can
	hardly generalize to unseen domains. Furthermore, we show that the current
	coreference resolution evaluation is clearly flawed by only evaluating on a
	specific split of a specific dataset in which there is a notable overlap
	between the training, development and test sets.},
  url       = {http://aclweb.org/anthology/P17-2003}
}

@InProceedings{stanojevic-simaan:2017:Short,
  author    = {Stanojevi\'{c}, Milo\v{s}  and  Sima'an, Khalil},
  title     = {Alternative Objective Functions for Training MT Evaluation Metrics},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {20--25},
  abstract  = {MT evaluation metrics are tested for correlation with human judgments either at
	the sentence- or the corpus-level. Trained metrics ignore corpus-level
	judgments and are trained for high sentence-level correlation only. We show
	that training only for one objective (sentence or corpus level), can not only
	harm the performance on the other objective, but it can also be suboptimal for
	the objective being optimized. To this end we present a metric trained for
	corpus-level and show empirical comparison against a metric trained for
	sentence-level exemplifying how their performance may vary per language pair,
	type and level of judgment. Subsequently we propose a model trained to optimize
	both objectives simultaneously and show that it is far more stable than--and on
	average outperforms--both models on both objectives.},
  url       = {http://aclweb.org/anthology/P17-2004}
}

@InProceedings{peyrard-ecklekohler:2017:Short,
  author    = {Peyrard, Maxime  and  Eckle-Kohler, Judith},
  title     = {A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {26--31},
  abstract  = {We present a new framework for evaluating extractive summarizers, which is
	based on a principled representation as optimization problem. We prove that
	every extractive summarizer can be decomposed into an objective function  and
	an optimization technique. We perform a comparative analysis and evaluation of
	several objective functions embedded in well-known summarizers regarding their
	correlation with human judgments. Our comparison of these correlations across
	two datasets yields surprising insights into the role and performance of
	objective functions in the different  summarizers.},
  url       = {http://aclweb.org/anthology/P17-2005}
}

@InProceedings{prudhommeaux-vansanten-gliner:2017:Short,
  author    = {Prud'hommeaux, Emily  and  van Santen, Jan  and  Gliner, Douglas},
  title     = {Vector space models for evaluating semantic fluency in autism},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {32--37},
  abstract  = {A common test administered during neurological examination is the semantic
	fluency test, in which the patient must list as many examples of a given
	semantic category as possible under timed conditions. Poor performance 
	is associated with neurological conditions characterized
	by impairments in executive function, such as dementia, schizophrenia, and
	autism
	spectrum disorder (ASD). Methods for analyzing semantic fluency responses at 
	the level of detail necessary to uncover these differences have typically
	relied on subjective manual annotation.
	In this paper, we explore automated approaches for scoring semantic fluency 
	responses that leverage ontological resources and distributional semantic
	models to characterize the semantic fluency responses 
	produced by young children with and without ASD. Using these methods, we find
	significant differences
	in the semantic fluency responses of children with ASD, demonstrating
	the utility of using objective methods for clinical language analysis.},
  url       = {http://aclweb.org/anthology/P17-2006}
}

@InProceedings{susanto-lu:2017:Short,
  author    = {Susanto, Raymond Hendy  and  Lu, Wei},
  title     = {Neural Architectures for Multilingual Semantic Parsing},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {38--44},
  abstract  = {In this paper, we address semantic parsing in a multilingual context. We train
	one multilingual model that is capable of parsing natural language sentences
	from multiple different languages into their corresponding formal semantic
	representations. We extend an existing sequence-to-tree model to a multi-task
	learning framework which shares the decoder for generating semantic
	representations. We report evaluation results on the multilingual GeoQuery
	corpus and introduce a new multilingual version of the ATIS corpus.},
  url       = {http://aclweb.org/anthology/P17-2007}
}

@InProceedings{malinin-EtAl:2017:Short,
  author    = {Malinin, Andrey  and  Ragni, Anton  and  Knill, Kate  and  Gales, Mark},
  title     = {Incorporating Uncertainty into Deep Learning for Spoken Language Assessment},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {45--50},
  abstract  = {There is a growing demand for automatic assessment of spoken English
	proficiency. These systems need to handle large variations in input
	data owing to the wide range of candidate skill levels and L1s, and
	errors from ASR. Some candidates will be a poor match
	to the training data set, undermining the validity of the predicted grade. For
	high stakes tests it is essential for such systems not only to grade well, but
	also to provide a measure
	of their uncertainty in their predictions, enabling rejection to human
	graders. Previous work examined Gaussian Process (GP) graders which, though
	successful, do not scale well with large data sets. Deep Neural Network (DNN)
	may also be used to provide uncertainty using Monte-Carlo Dropout (MCD). This
	paper proposes a novel method to yield uncertainty and compares it to GPs and
	DNNs with MCD. The proposed approach explicitly teaches a DNN to have low
	uncertainty on training data and high uncertainty on generated artificial data.
	On experiments conducted on data from the Business Language Testing Service
	(BULATS), the proposed approach is found to outperform GPs and DNNs with MCD in
	uncertainty-based rejection whilst achieving comparable grading performance.},
  url       = {http://aclweb.org/anthology/P17-2008}
}

@InProceedings{jurgens-tsvetkov-jurafsky:2017:Short,
  author    = {Jurgens, David  and  Tsvetkov, Yulia  and  Jurafsky, Dan},
  title     = {Incorporating Dialectal Variability for Socially Equitable Language Identification},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {51--57},
  abstract  = {Language identification (LID) is a critical first step for processing
	multilingual text.  Yet most LID systems are not designed to handle the
	linguistic diversity of global platforms like Twitter, where local dialects and
	rampant code-switching lead language classifiers to systematically miss
	minority dialect speakers and multilingual speakers.  We propose a new dataset
	and a character-based sequence-to-sequence model for LID designed to support
	dialectal and multilingual language varieties. Our model achieves
	state-of-the-art performance on multiple LID benchmarks.  Furthermore, in a
	case study using Twitter for health tracking,  our method substantially
	increases the availability of texts written by underrepresented populations, 
	enabling the development of "socially inclusive" NLP tools.},
  url       = {http://aclweb.org/anthology/P17-2009}
}

@InProceedings{jagfeld-ziering-vanderplas:2017:Short,
  author    = {Jagfeld, Glorianna  and  Ziering, Patrick  and  van der Plas, Lonneke},
  title     = {Evaluating Compound Splitters Extrinsically with Textual Entailment},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {58--63},
  abstract  = {Traditionally, compound splitters are evaluated intrinsically on gold-standard
	data or extrinsically on the task of statistical machine translation. We
	explore a novel way for the extrinsic evaluation of compound splitters, namely
	recognizing textual entailment. Compound splitting has great potential for this
	novel task that is both transparent and well-defined. Moreover, we show that it
	addresses certain aspects that are either ignored in intrinsic evaluations or
	compensated for by taskinternal mechanisms in statistical machine translation.
	We show significant improvements using different compound splitting methods on
	a German textual entailment dataset.},
  url       = {http://aclweb.org/anthology/P17-2010}
}

@InProceedings{gella-keller:2017:Short,
  author    = {Gella, Spandana  and  Keller, Frank},
  title     = {An Analysis of Action Recognition Datasets for Language and Vision Tasks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {64--71},
  abstract  = {A large amount of recent research has focused on 
	tasks that combine language and vision, resulting in
	a proliferation of datasets and methods. One such task is 
	action recognition, whose applications
	include image annotation, scene understanding and 
	image retrieval. In this survey, we categorize the 
	existing approaches based on how 
	they conceptualize this problem  
	and provide a detailed review of existing datasets, 
	highlighting their diversity as well as advantages and
	disadvantages. We focus on recently developed 
	datasets which link visual information with
	linguistic resources and provide a fine-grained syntactic and semantic 
	analysis of actions in images.},
  url       = {http://aclweb.org/anthology/P17-2011}
}

@InProceedings{eriguchi-tsuruoka-cho:2017:Short,
  author    = {Eriguchi, Akiko  and  Tsuruoka, Yoshimasa  and  Cho, Kyunghyun},
  title     = {Learning to Parse and Translate Improves Neural Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {72--78},
  abstract  = {There has been relatively little attention to incorporating linguistic prior to
	neural machine translation. Much of the previous work was further constrained
	to considering linguistic prior on the source side. In this paper, we propose a
	hybrid model, called NMT+RNNG, that learns to parse and translate by combining
	the recurrent neural network grammar into the attention-based neural machine
	translation. Our approach encourages the neural machine translation model to
	incorporate linguistic prior during training, and lets it translate on its own
	afterward. Extensive experiments with four language pairs show the
	effectiveness of the proposed NMT+RNNG.},
  url       = {http://aclweb.org/anthology/P17-2012}
}

@InProceedings{almodaresi-EtAl:2017:Short,
  author    = {Almodaresi, Fatemeh  and  Ungar, Lyle  and  Kulkarni, Vivek  and  Zakeri, Mohsen  and  Giorgi, Salvatore  and  Schwartz, H. Andrew},
  title     = {On the Distribution of Lexical Features at Multiple Levels of Analysis},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {79--84},
  abstract  = {Natural language processing has increasingly moved from modeling documents and
	words toward studying the people behind the language. This move to working with
	data at the user or community level has presented the field with different
	characteristics of linguistic data. In this paper, we empirically characterize
	various lexical distributions at different levels of analysis, showing that,
	while most features are decidedly sparse and non-normal at the message-level
	(as with traditional NLP), they follow the central limit theorem to become much
	more Log-normal or even Normal at the user- and county-levels. Finally, we
	demonstrate that modeling lexical features for the correct level of analysis
	leads to marked improvements in common social scientific prediction tasks.},
  url       = {http://aclweb.org/anthology/P17-2013}
}

@InProceedings{nisioi-EtAl:2017:Short,
  author    = {Nisioi, Sergiu  and  \v{S}tajner, Sanja  and  Ponzetto, Simone Paolo  and  Dinu, Liviu P.},
  title     = {Exploring Neural Text Simplification Models},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {85--91},
  abstract  = {We present the first attempt at using sequence to sequence neural networks to
	model text simplification (TS). Unlike the previously proposed automated TS
	systems, our neural text simplification (NTS) systems are able to
	simultaneously perform lexical simplification and content reduction. An
	extensive human evaluation of the output has shown that NTS systems achieve
	almost perfect                          grammaticality and meaning preservation of
	output
	sentences and
	higher level of simplification than the state-of-the-art automated TS systems},
  url       = {http://aclweb.org/anthology/P17-2014}
}

@InProceedings{dahlmeier:2017:Short,
  author    = {Dahlmeier, Daniel},
  title     = {On the Challenges of Translating NLP Research into Commercial Products},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {92--96},
  abstract  = {This paper highlights challenges in industrial research related to translating
	research in natural language processing into commercial products.
	While the interest in natural language processing from industry is
	significant, the transfer of research to commercial products is
	non-trivial and its challenges are often unknown to or underestimated
	by many researchers. I discuss current obstacles and provide
	suggestions for increasing the chances for translating research to commercial
	success based on my experience in industrial research.},
  url       = {http://aclweb.org/anthology/P17-2015}
}

@InProceedings{vstajner-EtAl:2017:Short,
  author    = {\v{S}tajner, Sanja  and  Franco-Salvador, Marc  and  Ponzetto, Simone Paolo  and  Rosso, Paolo  and  Stuckenschmidt, Heiner},
  title     = {Sentence Alignment Methods for Improving Text Simplification Systems},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {97--102},
  abstract  = {We provide several methods for sentence-alignment of texts with different
	complexity levels. Using the best of them, we sentence-align the Newsela
	corpora, thus providing large training materials for automatic text
	simplification (ATS) systems. We show that using this dataset, even the
	standard phrase-based statistical machine translation models for ATS can
	outperform the state-of-the-art ATS systems.},
  url       = {http://aclweb.org/anthology/P17-2016}
}

@InProceedings{jiang-kummerfeld-lasecki:2017:Short,
  author    = {Jiang, Youxuan  and  Kummerfeld, Jonathan K.  and  Lasecki, Walter S.},
  title     = {Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {103--109},
  abstract  = {Linguistically diverse datasets are critical for training and evaluating robust
	machine learning systems, but data collection is a costly process that often
	requires experts. Crowdsourcing the process of paraphrase generation is an
	effective means of expanding natural language datasets, but there has been
	limited analysis of the trade-offs that arise when designing tasks. In this
	paper, we present the first systematic study of the key factors in
	crowdsourcing paraphrase collection. We consider variations in instructions,
	incentives, data domains, and workflows. We manually analyzed paraphrases for
	correctness, grammaticality, and linguistic diversity. Our observations provide
	new insight into the trade-offs between accuracy and diversity in crowd
	responses that arise as a result of task design, providing guidance for future
	paraphrase generation procedures.},
  url       = {http://aclweb.org/anthology/P17-2017}
}

@InProceedings{qi-manning:2017:Short,
  author    = {Qi, Peng  and  Manning, Christopher D.},
  title     = {Arc-swift: A Novel Transition System for Dependency Parsing},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {110--117},
  abstract  = {Transition-based dependency parsers often need sequences of local shift and
	reduce operations to produce certain attachments. Correct individual decisions
	hence require global information about the sentence context and mistakes
	cause error propagation. This paper proposes a novel transition system,
	arc-swift, that enables direct attachments between tokens farther apart with a
	single transition. This allows the parser to leverage lexical information more
	directly in transition decisions. Hence, arc-swift can achieve significantly
	better performance with a very small beam size. Our parsers reduce error by
	3.7--7.6% relative to those using existing transition systems on the Penn
	Treebank dependency parsing task and English Universal Dependencies.},
  url       = {http://aclweb.org/anthology/P17-2018}
}

@InProceedings{cheng-lopez-lapata:2017:Short,
  author    = {Cheng, Jianpeng  and  Lopez, Adam  and  Lapata, Mirella},
  title     = {A Generative Parser with a Discriminative Recognition Algorithm},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {118--124},
  abstract  = {Generative models defining joint distributions over parse trees and sentences
	are useful for parsing and language modeling, but impose restrictions on the
	scope of features and are often outperformed by discriminative models. We
	propose a framework for parsing and language modeling which marries a
	generative model with a discriminative recognition model in an encoder-decoder
	setting. We provide interpretations of the framework based on expectation
	maximization and variational inference, and show that it enables parsing and
	language modeling within a single implementation. On the English Penn
	Treen-bank, our framework obtains competitive performance on constituency
	parsing while matching the state-of-the-art single- model language modeling
	score.},
  url       = {http://aclweb.org/anthology/P17-2019}
}

@InProceedings{wang-EtAl:2017:Short1,
  author    = {Wang, Weiyue  and  Alkhouli, Tamer  and  Zhu, Derui  and  Ney, Hermann},
  title     = {Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {125--131},
  abstract  = {Recently, the neural machine translation systems showed their promising
	performance and surpassed the phrase-based systems for most translation tasks.
	Retreating into conventional concepts machine translation while utilizing
	effective neural models is vital for comprehending the leap accomplished by
	neural machine translation over phrase-based methods. This work proposes a
	direct HMM with neural network-based lexicon and alignment models, which are
	trained jointly using the Baum-Welch algorithm. The direct HMM is applied to
	rerank the n-best list created by a state-of-the-art phrase-based translation
	system and it provides improvements by up to 1.0% Bleu scores on two different
	translation tasks.},
  url       = {http://aclweb.org/anthology/P17-2020}
}

@InProceedings{aharoni-goldberg:2017:Short,
  author    = {Aharoni, Roee  and  Goldberg, Yoav},
  title     = {Towards String-To-Tree Neural Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {132--140},
  abstract  = {We present a simple method to incorporate syntactic information about the
	target language in a neural machine translation system by translating into
	linearized, lexicalized constituency trees. An experiment on the WMT16
	German-English news translation task resulted in an improved BLEU score when
	compared to a syntax-agnostic NMT baseline trained on the same dataset. An
	analysis of the translations from the syntax-aware system shows that it
	performs more reordering during translation in comparison to the baseline. A
	small-scale human evaluation also showed an advantage to the syntax-aware
	system.},
  url       = {http://aclweb.org/anthology/P17-2021}
}

@InProceedings{reed-EtAl:2017:Short,
  author    = {Reed, Lena  and  Wu, Jiaqi  and  Oraby, Shereen  and  Anand, Pranav  and  Walker, Marilyn},
  title     = {Learning Lexico-Functional Patterns for First-Person Affect},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {141--147},
  abstract  = {Informal first-person narratives are a unique resource for computational mod-
	els of everyday events and people’s affective reactions to them. People
	blogging about their day tend not to explicitly say I am happy. Instead they
	describe situations from which other humans can readily infer their affective
	reactions. However current sentiment dictionaries are missing much of the
	information needed to make similar inferences. We build on recent work that
	models affect in terms of lexical predicate functions and affect on the
	predicate’s arguments. We present a method to learn proxies for these
	functions from first- person narratives. We construct a novel fine-grained test
	set, and show that the pat- terns we learn improve our ability to pre- dict
	first-person affective reactions to everyday events, from a Stanford sentiment
	baseline of .67F to .75F.},
  url       = {http://aclweb.org/anthology/P17-2022}
}

@InProceedings{shu-xu-liu:2017:Short,
  author    = {Shu, Lei  and  Xu, Hu  and  Liu, Bing},
  title     = {Lifelong Learning CRF for Supervised Aspect Extraction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {148--154},
  abstract  = {This paper makes a focused contribution to supervised aspect extraction. It
	shows that if the system has performed aspect extraction from many past domains
	and retained their results as knowledge, Conditional Random Fields (CRF) can
	leverage this knowledge in a lifelong learning manner to extract in a new
	domain markedly better than the traditional CRF without using this prior
	knowledge. The key innovation is that even after CRF training, the model can
	still improve its extraction with experiences in its applications.},
  url       = {http://aclweb.org/anthology/P17-2023}
}

@InProceedings{zhang-lease-wallace:2017:Short,
  author    = {Zhang, Ye  and  Lease, Matthew  and  Wallace, Byron C.},
  title     = {Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {155--160},
  abstract  = {A fundamental advantage of neural models
	for NLP is their ability to learn representations
	from scratch. However, in
	practice this often means ignoring existing
	external linguistic resources, e.g., WordNet
	or domain specific ontologies such
	as the Unified Medical Language System
	(UMLS). We propose a general, novel
	method for exploiting such resources via
	weight sharing. Prior work on weight
	sharing in neural networks has considered
	it largely as a means of model compression.
	In contrast, we treat weight sharing
	as a flexible mechanism for incorporating
	prior knowledge into neural models.
	We show that this approach consistently
	yields improved performance on classification tasks compared to baseline
	strategies that do not exploit weight sharing.},
  url       = {http://aclweb.org/anthology/P17-2024}
}

@InProceedings{fried-stern-klein:2017:Short,
  author    = {Fried, Daniel  and  Stern, Mitchell  and  Klein, Dan},
  title     = {Improving Neural Parsing by Disentangling Model Combination and Reranking Effects},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {161--166},
  abstract  = {Recent work has proposed several generative neural models for constituency
	parsing that achieve state-of-the-art results. Since direct search in these
	generative models is difficult, they have primarily been used to rescore
	candidate outputs from base parsers in which decoding is more straightforward. 
	We first present an algorithm for direct search in these generative models.  We
	then demonstrate that the rescoring results are at least partly due to implicit
	model combination rather than reranking effects.  Finally, we show that
	explicit model combination can improve performance even further, resulting in
	new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold
	data and 94.66 F1 when using external data.},
  url       = {http://aclweb.org/anthology/P17-2025}
}

@InProceedings{melamud-goldberger:2017:Short,
  author    = {Melamud, Oren  and  Goldberger, Jacob},
  title     = {Information-Theory Interpretation of the Skip-Gram Negative-Sampling Objective Function},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {167--171},
  abstract  = {In this paper we define a measure of dependency between two random variables,
	based on the Jensen-Shannon (JS) divergence between their joint distribution
	and the product of their marginal distributions. Then, we show that word2vec's
	skip-gram with negative sampling embedding algorithm finds the optimal
	low-dimensional approximation of this JS dependency measure between the words
	and their contexts. The gap between the optimal score and the low-dimensional
	approximation is demonstrated on a standard text corpus.},
  url       = {http://aclweb.org/anthology/P17-2026}
}

@InProceedings{kazi-thompson:2017:Short,
  author    = {Kazi, Michaeel  and  Thompson, Brian},
  title     = {Implicitly-Defined Neural Networks for Sequence Labeling},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {172--177},
  abstract  = {In this work, we propose a novel, implicitly-defined neural network
	architecture
	and describe a method to compute its components.
	The proposed architecture forgoes the causality assumption
	used to formulate recurrent neural networks
	and instead couples the hidden states of the network,
	allowing improvement on problems with complex, long-distance dependencies.
	Initial experiments demonstrate the new architecture outperforms both the
	Stanford Parser
	and baseline bidirectional networks on the Penn Treebank Part-of-Speech tagging
	task
	and a baseline bidirectional network on an additional artificial random biased
	walk task.},
  url       = {http://aclweb.org/anthology/P17-2027}
}

@InProceedings{ludusan-EtAl:2017:Short,
  author    = {Ludusan, Bogdan  and  Mazuka, Reiko  and  Bernard, Mathieu  and  Cristia, Alejandrina  and  Dupoux, Emmanuel},
  title     = {The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {178--183},
  abstract  = {This study explores the role of speech register and prosody for the task of
	word segmentation. Since these two factors are thought to play an important
	role in early language acquisition, we aim to quantify their contribution for
	this task. We study a Japanese corpus containing both infant- and
	adult-directed speech and we apply four different word segmentation models,
	with and without knowledge of prosodic boundaries. The results showed that the
	difference between registers is smaller than previously reported and that
	prosodic boundary information helps more adult- than infant-directed speech.},
  url       = {http://aclweb.org/anthology/P17-2028}
}

@InProceedings{wang-li-wang:2017:Short,
  author    = {Wang, Yizhong  and  Li, Sujian  and  Wang, Houfeng},
  title     = {A Two-Stage Parsing Method for Text-Level Discourse Analysis},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {184--188},
  abstract  = {Previous work introduced transition-based algorithms to form a unified
	architecture
	of parsing rhetorical structures (including span, nuclearity and relation), but
	did not achieve satisfactory performance. In this paper, we propose that
	transition-based model is more appropriate for parsing the naked discourse tree
	(i.e., identifying span and nuclearity) due to data sparsity. At the same time,
	we argue that relation labeling can benefit from naked tree structure and
	should be treated elaborately with consideration of three kinds of relations
	including within-sentence, across-sentence and across-paragraph relations.
	Thus, we design a pipelined two-stage parsing method for generating an RST tree
	from text. Experimental results show that our method achieves state-of-the-art
	performance, especially on span and nuclearity identification.},
  url       = {http://aclweb.org/anthology/P17-2029}
}

@InProceedings{sakaguchi-post-vandurme:2017:Short,
  author    = {Sakaguchi, Keisuke  and  Post, Matt  and  Van Durme, Benjamin},
  title     = {Error-repair Dependency Parsing for Ungrammatical Texts},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {189--195},
  abstract  = {We propose a new dependency parsing scheme which jointly parses a sentence and
	repairs grammatical errors by extending the non-directional transition-based
	formalism of Goldberg and Elhadad (2010) with three additional actions:
	SUBSTITUTE, DELETE, INSERT. Because these actions may cause an infinite loop in
	derivation, we also introduce simple constraints that ensure the parser
	termination. We evaluate our model with respect to dependency accuracy and
	grammaticality improvements for ungrammatical sentences, demonstrating the
	robustness and applicability of our scheme.},
  url       = {http://aclweb.org/anthology/P17-2030}
}

@InProceedings{libovicky-helcl:2017:Short,
  author    = {Libovick\'{y}, Jind\v{r}ich  and  Helcl, Jind\v{r}ich},
  title     = {Attention Strategies for Multi-Source Sequence-to-Sequence Learning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {196--202},
  abstract  = {Modeling attention in neural multi-source sequence-to-sequence learning remains
	a relatively unexplored area, despite its usefulness in tasks that incorporate
	multiple source languages or modalities.
	We propose two novel approaches to combine the outputs of attention mechanisms
	over each source sequence, flat and hierarchical.
	We compare the proposed methods with existing techniques and present results of
	systematic evaluation of those methods on the WMT16 Multimodal Translation and
	Automatic Post-editing tasks.
	We show that the proposed methods achieve competitive results on both tasks.},
  url       = {http://aclweb.org/anthology/P17-2031}
}

@InProceedings{hua-wang:2017:Short,
  author    = {Hua, Xinyu  and  Wang, Lu},
  title     = {Understanding and Detecting Supporting Arguments of Diverse Types},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {203--208},
  abstract  = {We investigate the problem of sentence-level supporting argument detection from
	relevant documents for user-specified claims. A dataset containing claims and
	associated citation articles is collected from online debate website
	idebate.org. We then manually label sentence-level supporting arguments from
	the documents along with their types as study, factual, opinion, or reasoning.
	We further characterize arguments of different types, and explore whether
	leveraging type information can facilitate the supporting arguments detection
	task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses
	features informed by argument types yields better performance than the same
	ranker trained without type information.},
  url       = {http://aclweb.org/anthology/P17-2032}
}

@InProceedings{rahimi-cohn-baldwin:2017:Short,
  author    = {Rahimi, Afshin  and  Cohn, Trevor  and  Baldwin, Timothy},
  title     = {A Neural Model for User Geolocation and Lexical Dialectology},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {209--216},
  abstract  = {We propose a simple yet effective text-based user geolocation model based on a
	neural network with one hidden layer, which achieves state of the art
	performance over three Twitter benchmark geolocation datasets, in addition to
	producing word                                and phrase embeddings in the hidden
	layer
	that we
	show to
	be
	useful for detecting dialectal terms. As part of our analysis of dialectal
	terms, we release DAREDS, a dataset for evaluating dialect term detection
	methods.},
  url       = {http://aclweb.org/anthology/P17-2033}
}

@InProceedings{suhr-EtAl:2017:Short,
  author    = {Suhr, Alane  and  Lewis, Mike  and  Yeh, James  and  Artzi, Yoav},
  title     = {A Corpus of Natural Language for Visual Reasoning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {217--223},
  abstract  = {We present a new visual reasoning language dataset, containing 92,244 pairs of
	examples of natural statements grounded in synthetic images with 3,962 unique
	sentences. We describe a method of crowdsourcing linguistically-diverse data,
	and present an analysis of our data. The data demonstrates a broad set of
	linguistic phenomena, requiring visual and set-theoretic reasoning. We
	experiment with various models, and show the data presents a strong challenge
	for future research.},
  url       = {http://aclweb.org/anthology/P17-2034}
}

@InProceedings{tourille-EtAl:2017:Short,
  author    = {Tourille, Julien  and  Ferret, Olivier  and  Neveol, Aurelie  and  Tannier, Xavier},
  title     = {Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {224--230},
  abstract  = {We present a neural architecture for containment relation identification
	between medical events and/or temporal expressions. We experiment on a corpus
	of de-identified clinical notes in English from the Mayo Clinic, namely the
	THYME corpus. Our model achieves an F-measure of 0.613 and outperforms the best
	result reported on this corpus to date.},
  url       = {http://aclweb.org/anthology/P17-2035}
}

@InProceedings{tian-EtAl:2017:Short,
  author    = {Tian, Zhiliang  and  Yan, Rui  and  Mou, Lili  and  Song, Yiping  and  Feng, Yansong  and  Zhao, Dongyan},
  title     = {How to Make Context More Useful? An Empirical Study on Context-Aware Neural Conversational Models},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {231--236},
  abstract  = {Generative conversational systems are attracting increasing attention in
	natural language processing (NLP). Recently, researchers have noticed the
	importance of context information in dialog processing, and built various
	models to utilize context. However, there is no systematic comparison to
	analyze how to use context effectively. In this paper, we conduct an empirical
	study to compare various models and investigate the effect of context
	information in dialog systems. We also propose a variant that explicitly
	weights context vectors by context-query relevance, outperforming the other
	baselines.},
  url       = {http://aclweb.org/anthology/P17-2036}
}

@InProceedings{braud-lacroix-sogaard:2017:Short,
  author    = {Braud, Chlo\'{e}  and  Lacroix, Oph\'{e}lie  and  S{\o}gaard, Anders},
  title     = {Cross-lingual and cross-domain discourse segmentation of entire documents},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {237--243},
  abstract  = {Discourse segmentation is a crucial step in building end-to-end discourse
	parsers. However, discourse segmenters only exist for a few languages and
	domains. Typically they only  detect intra-sentential segment boundaries,
	assuming gold standard sentence and token segmentation, and relying on
	high-quality syntactic parses and rich heuristics that are not generally
	available across languages and domains. In this paper, we propose statistical
	discourse segmenters for five languages and three domains that do not rely on
	gold pre-annotations.  We also consider the problem of learning discourse
	segmenters when no labeled data is available for a language. Our fully
	supervised system obtains 89.5% F1 for English newswire, with slight drops in
	performance on other domains, and we report supervised and unsupervised
	(cross-lingual) results for five languages in total.},
  url       = {http://aclweb.org/anthology/P17-2037}
}

@InProceedings{beigmanklebanov-gyawali-song:2017:Short,
  author    = {Beigman Klebanov, Beata  and  Gyawali, Binod  and  Song, Yi},
  title     = {Detecting Good Arguments in a Non-Topic-Specific Way: An Oxymoron?},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {244--249},
  abstract  = {Automatic identification of good arguments on a controversial topic has
	applications in civics and education, to name a few. While in the civics
	context it might be acceptable to create separate models for  each topic, in
	the context of              scoring of students' writing there is a preference for a
	single
	model that applies to all responses. Given that good arguments for one topic
	are likely to be irrelevant for another, is a single model for detecting good
	arguments a contradiction in terms? We investigate the extent to which it is
	possible to close the performance gap between topic-specific and across-topics
	models for identification of good arguments.},
  url       = {http://aclweb.org/anthology/P17-2038}
}

@InProceedings{wachsmuth-EtAl:2017:Short,
  author    = {Wachsmuth, Henning  and  Naderi, Nona  and  Habernal, Ivan  and  Hou, Yufang  and  Hirst, Graeme  and  Gurevych, Iryna  and  Stein, Benno},
  title     = {Argumentation Quality Assessment: Theory vs. Practice},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {250--255},
  abstract  = {Argumentation quality is viewed differently in argumentation theory and in
	practical assessment approaches. This paper studies to what extent the views
	match empirically. We find that most observations on quality phrased
	spontaneously are in fact adequately represented by theory. Even more, relative
	comparisons of arguments in practice correlate with absolute quality ratings
	based on theory. Our results clarify how the two views can learn from each
	other.},
  url       = {http://aclweb.org/anthology/P17-2039}
}

@InProceedings{ronnqvist-schenk-chiarcos:2017:Short,
  author    = {R\"{o}nnqvist, Samuel  and  Schenk, Niko  and  Chiarcos, Christian},
  title     = {A Recurrent Neural Model with Attention for the Recognition of Chinese Implicit Discourse Relations},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {256--262},
  abstract  = {We introduce an attention-based Bi-LSTM for Chinese implicit discourse
	relations and demonstrate that modeling argument pairs as a joint sequence can
	outperform word order-agnostic approaches. Our model benefits from a partial
	sampling scheme and is conceptually simple, yet achieves state-of-the-art
	performance on the Chinese Discourse Treebank. We also visualize its attention
	activity to illustrate the model's ability to selectively focus on the relevant
	parts of an input sequence.},
  url       = {http://aclweb.org/anthology/P17-2040}
}

@InProceedings{wang-EtAl:2017:Short2,
  author    = {Wang, Xinhao  and  Bruno, James  and  Molloy, Hillary  and  Evanini, Keelan  and  Zechner, Klaus},
  title     = {Discourse Annotation of Non-native Spontaneous Spoken Responses Using the Rhetorical Structure Theory Framework},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {263--268},
  abstract  = {The availability of the Rhetorical Structure Theory (RST) Discourse Treebank
	has spurred substantial research into discourse analysis of written texts;
	however, limited research has been conducted to date on RST annotation and
	parsing of spoken language, in particular, non-native spontaneous speech.
	Considering that the measurement of discourse coherence is typically a key
	metric in human scoring rubrics for assessments of spoken language, we
	initiated a research effort to obtain RST annotations of a large number of
	non-native spoken responses from a standardized assessment of academic English
	proficiency. The resulting inter-annotator kappa agreements on the three
	different levels of Span, Nuclearity, and Relation are 0.848, 0.766, and 0.653,
	respectively. Furthermore, a set of features was explored to evaluate the
	discourse structure of non-native spontaneous speech based on these
	annotations; the highest performing feature resulted in a correlation of 0.612
	with scores of discourse coherence provided by expert human raters.},
  url       = {http://aclweb.org/anthology/P17-2041}
}

@InProceedings{wu-EtAl:2017:Short,
  author    = {Wu, Changxing  and  Shi, Xiaodong  and  Chen, Yidong  and  Su, Jinsong  and  Wang, Boli},
  title     = {Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {269--274},
  abstract  = {We introduce a simple and effective method to learn discourse-specific word
	embeddings (DSWE) for implicit discourse relation recognition. Specifically,
	DSWE is learned by performing connective classification on massive explicit
	discourse data, and capable of capturing discourse relationships between words.
	On the PDTB data set, using DSWE as features achieves significant improvements
	over baselines.},
  url       = {http://aclweb.org/anthology/P17-2042}
}

@InProceedings{hirao-nishino-nagata:2017:Short,
  author    = {Hirao, Tsutomu  and  Nishino, Masaaki  and  Nagata, Masaaki},
  title     = {Oracle Summaries of Compressive Summarization},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {275--280},
  abstract  = {This paper derives an Integer Linear Programming (ILP) formulation to obtain an
	oracle summary of the compressive summarization paradigm in terms of ROUGE. The
	oracle summary is essential to reveal the upper bound performance of the
	paradigm. Experimental results on the DUC dataset showed that ROUGE scores of
	compressive oracles are significantly higher than those of extractive oracles
	and state-of-the-art summarization systems. These results reveal that
	compressive summarization is a promising  paradigm and encourage us to continue
	with the research to produce informative summaries.},
  url       = {http://aclweb.org/anthology/P17-2043}
}

@InProceedings{hasegawa-EtAl:2017:Short,
  author    = {Hasegawa, Shun  and  Kikuchi, Yuta  and  Takamura, Hiroya  and  Okumura, Manabu},
  title     = {Japanese Sentence Compression with a Large Training Dataset},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {281--286},
  abstract  = {In English, high-quality sentence compression models by deleting words have
	been trained on automatically created large training datasets. We work on
	Japanese sentence compression by a similar approach. To create a large Japanese
	training dataset, a method of creating English training dataset is modified
	based on the characteristics of the Japanese language. The created dataset is
	used to train Japanese sentence compression models based on the recurrent
	neural network.},
  url       = {http://aclweb.org/anthology/P17-2044}
}

@InProceedings{loyola-marresetaylor-matsuo:2017:Short,
  author    = {Loyola, Pablo  and  Marrese-Taylor, Edison  and  Matsuo, Yutaka},
  title     = {A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {287--292},
  abstract  = {We propose a model to automatically describe changes introduced in the source
	code of a program using natural language. Our method receives as input a set of
	code commits, which contains both the modifications and  message introduced by
	an user. These two modalities are used to train  an encoder-decoder
	architecture. We evaluated our approach on twelve real world open source
	projects from four different programming languages. Quantitative and
	qualitative results showed that the proposed approach can generate feasible and
	semantically sound descriptions not only in standard in-project settings, but
	also in a cross-project setting.},
  url       = {http://aclweb.org/anthology/P17-2045}
}

@InProceedings{wei-EtAl:2017:Short,
  author    = {Wei, Sam  and  Korostil, Igor  and  Nothman, Joel  and  Hachey, Ben},
  title     = {English Event Detection With Translated Language Features},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {293--298},
  abstract  = {We propose novel radical features from automatic translation for event
	extraction.
	Event detection is a complex language processing task for which it is expensive
	to collect training data, making generalisation challenging. 
	We derive meaningful subword features from automatic translations into target
	language.
	Results suggest this method is particularly useful when using languages with
	writing systems that facilitate easy decomposition into subword features, e.g.,
	logograms and Cangjie.
	The best result combines logogram features from Chinese and Japanese with
	syllable features from Korean, providing an additional 3.0 points f-score when
	added to state-of-the-art generalisation features on the TAC KBP 2015 Event
	Nugget task.},
  url       = {http://aclweb.org/anthology/P17-2046}
}

@InProceedings{savenkov-agichtein:2017:Short,
  author    = {Savenkov, Denis  and  Agichtein, Eugene},
  title     = {EviNets: Neural Networks for Combining Evidence Signals for Factoid Question Answering},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {299--304},
  abstract  = {A critical task for question answering is the final answer selection stage,
	which has to combine multiple signals available about each answer candidate.
	This paper proposes EviNets: a novel neural network architecture for factoid
	question answering. EviNets scores candidate answer entities by combining the
	available supporting evidence, e.g., structured knowledge bases and
	unstructured text documents. EviNets represents each piece of evidence with a
	dense embeddings vector, scores their relevance to the question, and aggregates
	the support for each candidate to predict their final scores. Each of the
	components is generic and allows plugging in a variety of models for semantic
	similarity scoring and information aggregation. We demonstrate the
	effectiveness of EviNets in experiments on the existing TREC QA and WikiMovies
	benchmarks, and on the new Yahoo! Answers dataset introduced in this paper.
	EviNets can be extended to other information types and could facilitate future
	work on combining evidence signals for joint reasoning in question answering.},
  url       = {http://aclweb.org/anthology/P17-2047}
}

@InProceedings{wolfe-dredze-vandurme:2017:Short,
  author    = {Wolfe, Travis  and  Dredze, Mark  and  Van Durme, Benjamin},
  title     = {Pocket Knowledge Base Population},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {305--310},
  abstract  = {Existing Knowledge Base Population methods extract relations from a closed
	relational schema with limited coverage leading to sparse KBs. We propose
	Pocket Knowledge Base Population (PKBP), the task of dynamically constructing a
	KB of entities related to a query and finding the best characterization of
	relationships between entities. We describe novel Open Information Extraction
	methods which leverage the PKB to find informative trigger words. We evaluate
	using existing KBP shared-task data as well anew annotations collected for this
	work. Our methods produce high quality KB from just text with many more
	entities and relationships than existing KBP systems.},
  url       = {http://aclweb.org/anthology/P17-2048}
}

@InProceedings{khot-sabharwal-clark:2017:Short,
  author    = {Khot, Tushar  and  Sabharwal, Ashish  and  Clark, Peter},
  title     = {Answering Complex Questions Using Open Information Extraction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {311--316},
  abstract  = {While there has been substantial progress in factoid question-answering (QA),
	answering complex questions remains challenging, typically requiring both a
	large body of knowledge and inference techniques. Open Information Extraction
	(Open IE) provides a way to generate semi-structured knowledge for QA, but to
	date such knowledge has only been used to answer simple questions with
	retrieval-based methods. We overcome this limitation by presenting a method for
	reasoning with Open IE knowledge, allowing more complex questions to be
	handled. Using a recently proposed support graph optimization framework for QA,
	we develop a new inference model for Open IE, in particular one that can work
	effectively with multiple short facts, noise, and the relational structure of
	tuples. Our model significantly outperforms a state-of-the-art structured
	solver on complex questions of varying difficulty, while also removing the
	reliance on manually curated knowledge.},
  url       = {http://aclweb.org/anthology/P17-2049}
}

@InProceedings{saha-pal-mausam:2017:Short,
  author    = {Saha, Swarnadeep  and  Pal, Harinder  and  Mausam},
  title     = {Bootstrapping for Numerical Open IE},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {317--323},
  abstract  = {We design and release BONIE, the first open numerical relation extractor, for
	extracting Open IE tuples where one of the arguments is a number or a
	quantity-unit phrase. BONIE uses bootstrapping to learn the specific dependency
	patterns that express numerical relations in a sentence. BONIE’s novelty lies
	in task-specific customizations, such as inferring implicit relations, which
	are clear due to context such as units (for e.g., ‘square kilometers’
	suggests
	area, even if the word ‘area’ is missing in the sentence). BONIE obtains
	1.5x yield and 15 point precision gain on numerical facts over a
	state-of-the-art Open IE system.},
  url       = {http://aclweb.org/anthology/P17-2050}
}

@InProceedings{komninos-manandhar:2017:Short,
  author    = {Komninos, Alexandros  and  Manandhar, Suresh},
  title     = {Feature-Rich Networks for Knowledge Base Completion},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {324--329},
  abstract  = {We propose jointly modelling Knowledge Bases and aligned text with Feature-Rich
	Networks. Our models perform Knowledge Base Completion by learning to represent
	and compose diverse feature types from partially aligned and noisy resources.
	We perform experiments on Freebase utilizing additional entity type information
	and syntactic textual relations. Our evaluation suggests that the proposed
	models can better incorporate side information than previously proposed
	combinations of bilinear models with convolutional neural networks, showing
	large improvements when scoring the plausibility of unobserved facts with
	associated textual mentions.},
  url       = {http://aclweb.org/anthology/P17-2051}
}

@InProceedings{rabinovich-klein:2017:Short,
  author    = {Rabinovich, Maxim  and  Klein, Dan},
  title     = {Fine-Grained Entity Typing with High-Multiplicity Assignments},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {330--334},
  abstract  = {As entity type systems become richer and more fine-grained, we expect the
	number of types assigned to a given entity to increase. However, most
	fine-grained typing work has focused on datasets that exhibit a low degree of
	type multiplicity. In this paper, we consider the high-multiplicity regime
	inherent in data sources such as Wikipedia that have semi-open type systems. We
	introduce a set-prediction approach to this problem and show that our model
	outperforms unstructured baselines on a new Wikipedia-based fine-grained typing
	corpus.},
  url       = {http://aclweb.org/anthology/P17-2052}
}

@InProceedings{ma-EtAl:2017:Short1,
  author    = {Ma, Mingbo  and  Huang, Liang  and  Xiang, Bing  and  Zhou, Bowen},
  title     = {Group Sparse CNNs for Question Classification with Answer Sets},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {335--340},
  abstract  = {Question classification is an important task with wide applications.
	However, traditional techniques treat questions as general sentences, ignoring
	the corresponding answer data.
	In order to consider answer information into question modeling, 
	we first introduce novel group sparse autoencoders 
	which refine question representation by utilizing group information in the
	answer set.
	We then propose novel group sparse CNNs 
	which naturally learn question representation with respect
	to their answers by implanting group sparse autoencoders into traditional CNNs.
	The proposed model significantly outperform strong baselines on four datasets.},
  url       = {http://aclweb.org/anthology/P17-2053}
}

@InProceedings{augenstein-sogaard:2017:Short,
  author    = {Augenstein, Isabelle  and  S{\o}gaard, Anders},
  title     = {Multi-Task Learning of Keyphrase Boundary Classification},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {341--346},
  abstract  = {Keyphrase boundary classification (KBC) is the task of detecting keyphrases in
	scientific articles and labelling them with respect to predefined types.
	Although important in practice, this task is so far underexplored, partly due
	to the lack of labelled data. 
	To overcome this, we explore several auxiliary tasks, including semantic
	super-sense tagging and identification of multi-word expressions, and cast the
	task as a multi-task learning problem with deep recurrent neural networks. Our
	multi-task models perform significantly better than previous state of the art
	approaches on two scientific KBC datasets, particularly for long keyphrases.},
  url       = {http://aclweb.org/anthology/P17-2054}
}

@InProceedings{mirza-EtAl:2017:Short,
  author    = {Mirza, Paramita  and  Razniewski, Simon  and  Darari, Fariz  and  Weikum, Gerhard},
  title     = {Cardinal Virtues: Extracting Relation Cardinalities from Text},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {347--351},
  abstract  = {Information extraction (IE) from text has largely focused on relations between
	individual entities, such as who has won which award. However, some facts are
	never fully mentioned, and no IE method has perfect recall. Thus, it is
	beneficial to also tap contents about the cardinalities of these relations, for
	example, how many awards someone has won. We introduce this novel problem of
	extracting cardinalities and discusses the specific challenges that set it
	apart from standard IE. We present a distant supervision method using
	conditional random fields. A preliminary evaluation results in precision
	between 3% and 55%, depending on the difficulty of relations.},
  url       = {http://aclweb.org/anthology/P17-2055}
}

@InProceedings{stanovsky-EtAl:2017:Short,
  author    = {Stanovsky, Gabriel  and  Eckle-Kohler, Judith  and  Puzikov, Yevgeniy  and  Dagan, Ido  and  Gurevych, Iryna},
  title     = {Integrating Deep Linguistic Features in Factuality Prediction over Unified Datasets},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {352--357},
  abstract  = {Previous models for the assessment of commitment towards a predicate in a
	sentence (also known as factuality prediction) were trained and tested against
	a specific annotated dataset, subsequently limiting the generality of their
	results. In this work we propose an intuitive method for mapping three
	previously annotated corpora onto a single factuality scale, thereby enabling
	models to be tested across these corpora. In addition, we design a novel model
	for factuality prediction by first extending a previous rule-based factuality
	prediction system and applying it over an abstraction of dependency trees, and
	then using the output of this system in a supervised classifier. We show that
	this model outperforms previous methods on all three datasets. We make both the
	unified factuality corpus and our new model publicly available.},
  url       = {http://aclweb.org/anthology/P17-2056}
}

@InProceedings{das-EtAl:2017:Short,
  author    = {Das, Rajarshi  and  Zaheer, Manzil  and  Reddy, Siva  and  McCallum, Andrew},
  title     = {Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {358--365},
  abstract  = {Existing question answering methods infer answers either from a knowledge base
	or from raw text. 
	While knowledge base (KB) methods are good at answering compositional
	questions, their performance is often affected by the incompleteness of the KB.
	Au contraire, 
	web text contains millions of facts that are absent in the KB, however in an
	unstructured form. Universal schema can support reasoning on the union of
	both structured KBs and unstructured text by aligning them in a common embedded
	space. In this paper we extend universal schema to natural language question
	answering, employing Memory networks to attend to the large body of
	facts in the combination of text and KB.
	Our models can be trained in an end-to-end fashion on question-answer pairs.
	Evaluation results on Spades fill-in-the-blank question answering dataset show
	that exploiting universal schema for question answering is better than using
	either a KB or text alone. 
	This model also outperforms the current state-of-the-art by 8.5 F1 points.},
  url       = {http://aclweb.org/anthology/P17-2057}
}

@InProceedings{goyal-dyer-bergkirkpatrick:2017:Short,
  author    = {Goyal, Kartik  and  Dyer, Chris  and  Berg-Kirkpatrick, Taylor},
  title     = {Differentiable Scheduled Sampling for Credit Assignment},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {366--371},
  abstract  = {We demonstrate that a continuous relaxation of the argmax operation can be used
	to create a differentiable approximation to greedy decoding in
	sequence-to-sequence (seq2seq) models. By incorporating this approximation into
	the scheduled sampling training procedure--a well-known technique for
	correcting exposure bias--we introduce a new training objective that is
	continuous and differentiable everywhere and can provide informative gradients
	near points where previous decoding decisions change their value. By using a
	related approximation, we also demonstrate  a similar approach to sampled-based
	training. We show that our approach outperforms both standard cross-entropy
	training and scheduled sampling procedures in two sequence prediction tasks:
	named entity recognition and machine translation.},
  url       = {http://aclweb.org/anthology/P17-2058}
}

@InProceedings{guo:2017:Short,
  author    = {Guo, Hongyu},
  title     = {A Deep Network with Visual Text Composition Behavior},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {372--377},
  abstract  = {While natural languages are compositional, how state-of-the-art neural models
	achieve compositionality is still unclear. We propose a deep network, which 
	not only achieves competitive accuracy for text classification, but also 
	exhibits compositional behavior. That is, while creating hierarchical 
	representations of a piece of text, such as a sentence, the lower layers of the
	network distribute their layer-specific attention weights to individual words.
	In contrast, the higher layers compose meaningful phrases and clauses, whose
	lengths increase as the networks get deeper until fully composing the 
	sentence.},
  url       = {http://aclweb.org/anthology/P17-2059}
}

@InProceedings{zhou-EtAl:2017:Short1,
  author    = {Zhou, Long  and  Hu, Wenpeng  and  Zhang, Jiajun  and  Zong, Chengqing},
  title     = {Neural System Combination for Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {378--384},
  abstract  = {Neural machine translation (NMT) becomes a new approach to machine translation
	and generates much more fluent results compared to statistical machine
	translation (SMT). However, SMT is usually better than NMT in translation
	adequacy. It is therefore a promising direction to combine the advantages of
	both NMT and SMT. In this paper, we propose a neural system combination
	framework leveraging multi-source NMT, which takes as input the outputs of NMT
	and SMT systems and produces the final translation. Extensive experiments on
	the Chinese-to-English translation task show that our model archives
	significant improvement by 5.3 BLEU points over the best single system output
	and 3.4 BLEU points over the state-of-the-art traditional system combination
	methods.},
  url       = {http://aclweb.org/anthology/P17-2060}
}

@InProceedings{chu-dabre-kurohashi:2017:Short,
  author    = {Chu, Chenhui  and  Dabre, Raj  and  Kurohashi, Sadao},
  title     = {An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {385--391},
  abstract  = {In this paper, we propose a novel domain adaptation method named "mixed fine
	tuning'' for neural machine translation (NMT). We combine two existing
	approaches namely fine tuning and multi domain NMT. We first train an NMT model
	on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus
	which is a mix of the in-domain and out-of-domain corpora. All corpora are
	augmented with artificial tags to indicate specific domains. We empirically
	compare our proposed method against fine tuning and multi domain methods and
	discuss its benefits and shortcomings.},
  url       = {http://aclweb.org/anthology/P17-2061}
}

@InProceedings{marie-fujita:2017:Short,
  author    = {Marie, Benjamin  and  Fujita, Atsushi},
  title     = {Efficient Extraction of Pseudo-Parallel Sentences from Raw Monolingual Data Using Word Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {392--398},
  abstract  = {We propose a new method for extracting pseudo-parallel sentences from a pair of
	large monolingual corpora, without relying on any document-level information.
	Our method first exploits word embeddings in order to efficiently evaluate
	trillions of candidate sentence pairs and then a classifier to find the most
	reliable ones. We report significant improvements in domain adaptation for
	statistical machine translation when using a translation model trained on the
	sentence pairs extracted from in-domain monolingual corpora.},
  url       = {http://aclweb.org/anthology/P17-2062}
}

@InProceedings{malmasi-dras:2017:Short,
  author    = {Malmasi, Shervin  and  Dras, Mark},
  title     = {Feature Hashing for Language and Dialect Identification},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {399--403},
  abstract  = {We evaluate feature hashing for language identification (LID), a method not
	previously used for this task. Using a standard dataset, we first show that
	while feature performance is high, LID data is highly dimensional and mostly
	sparse (>99.5%) as it includes large vocabularies for many languages; memory
	requirements grow as languages are added. Next we apply hashing using various
	hash sizes, demonstrating that there is no performance loss with dimensionality
	reductions of up to 86%. We also show that using an ensemble of low-dimension
	hash-based classifiers further boosts performance. Feature hashing is highly
	useful for LID and holds great promise for future work in this area.},
  url       = {http://aclweb.org/anthology/P17-2063}
}

@InProceedings{shiue-huang-chen:2017:Short,
  author    = {Shiue, Yow-Ting  and  Huang, Hen-Hsen  and  Chen, Hsin-Hsi},
  title     = {Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {404--410},
  abstract  = {Selecting appropriate words to compose a sentence is one common problem faced
	by non-native Chinese learners. In this paper, we propose (bidirectional) LSTM
	sequence labeling models and explore various features to detect word usage
	errors in Chinese sentences. By combining CWINDOW word embedding features and
	POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and
	MRR 0.6789 on the HSK dataset. For 80.79% of the test data, the model ranks the
	ground-truth within the top two at position level.},
  url       = {http://aclweb.org/anthology/P17-2064}
}

@InProceedings{ryskina-EtAl:2017:Short,
  author    = {Ryskina, Maria  and  Alpert-Abrams, Hannah  and  Garrette, Dan  and  Berg-Kirkpatrick, Taylor},
  title     = {Automatic Compositor Attribution in the First Folio of Shakespeare},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {411--416},
  abstract  = {Compositor attribution, the clustering of pages in a historical printed
	document by the individual who set the type, is a bibliographic task that
	relies on analysis of orthographic variation and inspection of visual details
	of the printed page. In this paper, we introduce a novel unsupervised model
	that jointly describes the textual and visual features needed to distinguish
	compositors. Applied to images of Shakespeare's First Folio, our model predicts
	attributions that agree with the manual judgements of bibliographers with an
	accuracy of 87%, even on text that is the output of OCR.},
  url       = {http://aclweb.org/anthology/P17-2065}
}

@InProceedings{yoshikawa-shigeto-takeuchi:2017:Short,
  author    = {Yoshikawa, Yuya  and  Shigeto, Yutaro  and  Takeuchi, Akikazu},
  title     = {STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {417--421},
  abstract  = {In recent years, automatic generation of image descriptions (captions), that
	is, image captioning, has attracted a great deal of attention.
	In this paper, we particularly consider generating Japanese captions for
	images.
	Since most available caption datasets have been constructed for English
	language, there are few datasets for Japanese.
	To tackle this problem, we construct a large-scale Japanese image caption
	dataset based on images from MS-COCO, which is called STAIR Captions.
	STAIR Captions consists of 820,310 Japanese captions for 164,062 images.
	In the experiment, we show that a neural network trained using STAIR Captions
	can generate more natural and better Japanese captions, compared to those
	generated using English-Japanese machine translation after generating English
	captions.},
  url       = {http://aclweb.org/anthology/P17-2066}
}

@InProceedings{wang:2017:Short,
  author    = {Wang, William Yang},
  title     = {"Liar, Liar Pants on Fire": A New Benchmark Dataset for Fake News Detection},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {422--426},
  abstract  = {Automatic fake news detection is a challenging problem in deception detection,
	and it has tremendous real-world political and social impacts. However,
	statistical approaches to combating fake news has been dramatically limited by
	the lack of labeled benchmark datasets. In this paper, we present LIAR: a new,
	publicly available dataset for fake news detection. We collected a decade-long,
	12.8K manually labeled short statements in various contexts from
	PolitiFact.com, which provides detailed analysis report and links to source
	documents for each case. This dataset can be used for fact-checking research as
	well. Notably, this new dataset is an order of magnitude larger than previously
	largest public fake news datasets of similar type. Empirically, we investigate
	automatic fake news detection based on surface-level linguistic patterns. We
	have designed a novel, hybrid convolutional neural network to integrate
	meta-data with text. We show that this hybrid approach can improve a text-only
	deep learning model.},
  url       = {http://aclweb.org/anthology/P17-2067}
}

@InProceedings{kato-shindo-matsumoto:2017:Short,
  author    = {Kato, Akihiko  and  Shindo, Hiroyuki  and  Matsumoto, Yuji},
  title     = {English Multiword Expression-aware Dependency Parsing Including Named Entities},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {427--432},
  abstract  = {Because syntactic structures and spans of multiword expressions (MWEs) are
	independently annotated in many English syntactic corpora, they are generally
	inconsistent with respect to one another, which is harmful to the
	implementation of an aggregate system. In this work, we construct a corpus that
	ensures consistency between dependency structures and MWEs, including named
	entities. Further, we explore models that predict both MWE-spans and an
	MWE-aware dependency structure. Experimental results show that our joint model
	using additional MWE-span features achieves an MWE recognition improvement of
	1.35 points over a pipeline model.},
  url       = {http://aclweb.org/anthology/P17-2068}
}

@InProceedings{kober-EtAl:2017:Short,
  author    = {Kober, Thomas  and  Weeds, Julie  and  Reffin, Jeremy  and  Weir, David},
  title     = {Improving Semantic Composition with Offset Inference},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {433--440},
  abstract  = {Count-based distributional semantic models suffer from sparsity due to
	unobserved but plausible co-occurrences in any text collection. This problem is
	amplified for models like Anchored Packed Trees (APTs), that take the
	grammatical type of a co-occurrence into account. We therefore introduce a
	novel form of distributional inference that exploits the rich type structure in
	APTs and infers missing data by the same mechanism that is used for semantic
	composition.},
  url       = {http://aclweb.org/anthology/P17-2069}
}

@InProceedings{fadaee-bisazza-monz:2017:Short1,
  author    = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
  title     = {Learning Topic-Sensitive Word Representations},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {441--447},
  abstract  = {Distributed word representations are widely used for modeling words in NLP
	tasks. Most of the existing models generate one representation per word and do
	not consider different meanings of a word.   
	We present two approaches to learn multiple topic-sensitive representations per
	word by using Hierarchical Dirichlet Process. We observe that by modeling
	topics and integrating topic distributions for each document  we obtain
	representations that are able to distinguish between different meanings of a
	given word.
	Our models yield statistically significant improvements for the lexical
	substitution task 
	indicating that commonly used single word representations, even when combined
	with contextual information, are insufficient for this task.},
  url       = {http://aclweb.org/anthology/P17-2070}
}

@InProceedings{szymanski:2017:Short,
  author    = {Szymanski, Terrence},
  title     = {Temporal Word Analogies: Identifying Lexical Replacement with Diachronic Word Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {448--453},
  abstract  = {This paper introduces the concept of temporal word analogies: pairs of words
	which occupy the same semantic space at different points in time. One
	well-known property of word embeddings is that they are able to effectively
	model traditional word analogies (“word w1 is to word w2 as word w3 is to
	word w4”) through vector addition. Here, I show that temporal word analogies
	(“word w1 at time tα is like word w2 at time tβ”) can effectively be
	modeled with diachronic word embeddings, provided that the independent
	embedding spaces from each time period are appropriately transformed into a
	common vector space. When applied to a diachronic corpus of news articles, this
	method is able to identify temporal word analogies such as “Ronald Reagan in
	1987 is like Bill Clinton in 1997”, or “Walkman in 1987 is like iPod in
	2007”.},
  url       = {http://aclweb.org/anthology/P17-2071}
}

@InProceedings{elrazzaz-EtAl:2017:Short,
  author    = {Elrazzaz, Mohammed  and  Elbassuoni, Shady  and  Shaban, Khaled  and  Helwe, Chadi},
  title     = {Methodical Evaluation of Arabic Word Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {454--458},
  abstract  = {Many unsupervised learning techniques have been proposed to obtain meaningful
	representations of words from text. In this study, we evaluate these various
	techniques when used to generate Arabic word embeddings. We first build a
	benchmark for the Arabic language that can be utilized to perform intrinsic
	evaluation of different word embeddings. We then perform additional extrinsic
	evaluations of the embeddings based on two NLP tasks.},
  url       = {http://aclweb.org/anthology/P17-2072}
}

@InProceedings{rashkin-EtAl:2017:Short,
  author    = {Rashkin, Hannah  and  Bell, Eric  and  Choi, Yejin  and  Volkova, Svitlana},
  title     = {Multilingual Connotation Frames: A Case Study on Social Media for Targeted Sentiment Analysis and Forecast},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {459--464},
  abstract  = {People around the globe respond to major real world events through social
	media.              To study targeted public sentiments across many languages and
	geographic locations, we introduce multilingual connotation frames: an
	extension from English connotation frames of Rashkin et al. (2016) with 10
	additional European languages, focusing on the implied sentiments among event
	participants engaged in a frame. As a case study, we present large scale
	analysis on targeted public sentiments toward salient events and entities using
	1.2 million multilingual connotation frames extracted from Twitter.},
  url       = {http://aclweb.org/anthology/P17-2073}
}

@InProceedings{kiritchenko-mohammad:2017:Short,
  author    = {Kiritchenko, Svetlana  and  Mohammad, Saif},
  title     = {Best-Worst Scaling More Reliable than Rating Scales: A Case Study on Sentiment Intensity Annotation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {465--470},
  abstract  = {Rating scales are a widely used method for data annotation; however, they
	present several challenges, such as difficulty in maintaining inter- and
	intra-annotator consistency. Best--worst scaling (BWS) is an alternative
	method of annotation that is claimed to produce high-quality annotations while
	keeping the required number of annotations similar to that of rating scales.
	However, the veracity of this claim has never been systematically established.
	Here for the first time, we set up an experiment that directly compares the
	rating scale method with BWS. We show that with the same total number of
	annotations, BWS produces significantly more reliable results than the rating
	scale.},
  url       = {http://aclweb.org/anthology/P17-2074}
}

@InProceedings{kim-EtAl:2017:Short,
  author    = {Kim, Sunghwan Mac  and  Xu, Qiongkai  and  Qu, Lizhen  and  Wan, Stephen  and  Paris, Cecile},
  title     = {Demographic Inference on Twitter using Recursive Neural Networks},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {471--477},
  abstract  = {In social media, demographic inference is a critical task in order to gain a
	better understanding of a cohort and to facilitate interacting with one's
	audience. Most previous work has made independence assumptions over
	topological, textual and label information on social networks. In this work, we
	employ recursive neural networks to break down these independence assumptions
	to obtain inference about demographic characteristics on Twitter. We show that
	our model performs better than existing models including the state-of-the-art.},
  url       = {http://aclweb.org/anthology/P17-2075}
}

@InProceedings{vijayaraghavan-vosoughi-roy:2017:Short,
  author    = {Vijayaraghavan, Prashanth  and  Vosoughi, Soroush  and  Roy, Deb},
  title     = {Twitter Demographic Classification Using Deep Multi-modal Multi-task Learning},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {478--483},
  abstract  = {Twitter should be an ideal place to get a fresh read on how different issues
	are playing with the public, one that's potentially more reflective of
	democracy in this new media age than traditional polls. Pollsters typically ask
	people a fixed set of questions, while in social media people use their own
	voices to speak about whatever is on their minds. However, the demographic
	distribution of users on Twitter is not representative of the general
	population. In this paper, we present a demographic classifier for gender, age,
	political orientation and location on Twitter. We collected and curated a
	robust Twitter demographic dataset for this task. Our classifier uses a deep
	multi-modal multi-task learning architecture to reach a state-of-the-art
	performance, achieving an F1-score of 0.89, 0.82, 0.86, and 0.68 for gender,
	age, political orientation, and location respectively.},
  url       = {http://aclweb.org/anthology/P17-2076}
}

@InProceedings{zhan-EtAl:2017:Short,
  author    = {Zhan, Xueying  and  Wang, Yaowei  and  Rao, Yanghui  and  Xie, Haoran  and  Li, Qing  and  Wang, Fu Lee  and  Wong, Tak-Lam},
  title     = {A Network Framework for Noisy Label Aggregation in Social Media},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {484--490},
  abstract  = {This paper focuses on the task of noisy label aggregation in social media,
	where users with different social or culture backgrounds may annotate invalid
	or malicious tags for documents. To aggregate noisy labels at a small cost, a
	network framework is proposed by calculating the matching degree of a
	document's topics and the annotators' meta-data. Unlike using the
	back-propagation algorithm, a probabilistic inference approach is adopted to
	estimate network parameters. Finally, a new simulation method is designed for
	validating the effectiveness of the proposed framework in aggregating noisy
	labels.},
  url       = {http://aclweb.org/anthology/P17-2077}
}

@InProceedings{vandergoot-vannoord:2017:Short,
  author    = {van der Goot, Rob  and  van Noord, Gertjan},
  title     = {Parser Adaptation for Social Media by Integrating Normalization},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {491--497},
  abstract  = {This work explores different approaches of using normalization for parser
	adaptation.  Traditionally, normalization is used as separate pre-processing
	step. We show that integrating the normalization model into the
	parsing algorithm is more beneficial. This way, multiple normalization
	candidates can be leveraged, which improves parsing performance on social
	media.
	We test this hypothesis by modifying the Berkeley parser; out-of-the-box it
	achieves an F1 score of 66.52.                          Our integrated approach
	reaches a
	significant
	improvement with an F1 score of 67.36, while using the best normalization
	sequence results in an F1 score of only 66.94.},
  url       = {http://aclweb.org/anthology/P17-2078}
}

@InProceedings{qiu-EtAl:2017:Short,
  author    = {Qiu, Minghui  and  Li, Feng-Lin  and  Wang, Siyu  and  Gao, Xing  and  Chen, Yan  and  Zhao, Weipeng  and  Chen, Haiqing  and  Huang, Jun  and  Chu, Wei},
  title     = {AliMe Chat: A Sequence to Sequence and Rerank based Chatbot Engine},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {498--503},
  abstract  = {We propose AliMe Chat, an open-domain chatbot engine that integrates the joint
	results of Information Retrieval (IR) and Sequence to Sequence (Seq2Seq) based
	generation models. AliMe Chat uses an attentive Seq2Seq based rerank model to
	optimize the joint results. Extensive experiments show our engine outperforms
	both IR and generation based models. We launch AliMe Chat for a real-world
	industrial application and observe better results than another public chatbot.},
  url       = {http://aclweb.org/anthology/P17-2079}
}

@InProceedings{shen-EtAl:2017:Short,
  author    = {Shen, Xiaoyu  and  Su, Hui  and  Li, Yanran  and  Li, Wenjie  and  Niu, Shuzi  and  Zhao, Yang  and  Aizawa, Akiko  and  Long, Guoping},
  title     = {A Conditional Variational Framework for Dialog Generation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {504--509},
  abstract  = {Deep latent variable models have been shown to facilitate the response
	generation for open-domain dialog systems. However, these latent variables are
	highly randomized, leading to uncontrollable generated responses. In this
	paper, we propose a framework allowing conditional response generation based on
	specific attributes. These attributes can be either manually assigned or
	automatically detected. Moreover, the dialog states for both speakers are
	modeled separately in order to reflect personal features. We validate this
	framework on two different scenarios, where the attribute refers to genericness
	and sentiment states respectively. The experiment result testified the
	potential of our model, where meaningful responses can be generated in
	accordance with the specified attributes.},
  url       = {http://aclweb.org/anthology/P17-2080}
}

@InProceedings{min-seo-hajishirzi:2017:Short,
  author    = {Min, Sewon  and  Seo, Minjoon  and  Hajishirzi, Hannaneh},
  title     = {Question Answering through Transfer Learning from Large Fine-grained Supervision Data},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {510--517},
  abstract  = {We show that the task of question answering (QA) can significantly benefit from
	the transfer learning of models trained on a different large, fine-grained QA
	dataset. We achieve the state of the art in two well-studied QA datasets,
	WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique
	from SQuAD. For WikiQA, our model outperforms the previous best model by more
	than 8%. We demonstrate that finer supervision provides better guidance for
	learning lexical and syntactic information than coarser supervision, through
	quantitative results and visual analysis. We also show that a similar transfer
	learning procedure  achieves  the state of the art on an entailment task.},
  url       = {http://aclweb.org/anthology/P17-2081}
}

@InProceedings{abad-nabi-moschitti:2017:Short,
  author    = {Abad, Azad  and  Nabi, Moin  and  Moschitti, Alessandro},
  title     = {Self-Crowdsourcing Training for Relation Extraction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {518--523},
  abstract  = {In this paper we introduce a self-training strategy for crowdsourcing.
	The training examples are automatically selected to train the crowd workers.
	Our experimental results show an impact of 5% Improvement in terms of F1 for
	relation extraction task, compared to the method based on distant supervision.},
  url       = {http://aclweb.org/anthology/P17-2082}
}

@InProceedings{tran-haffari-zukerman:2017:Short,
  author    = {Tran, Quan Hung  and  Haffari, Gholamreza  and  Zukerman, Ingrid},
  title     = {A Generative Attentional Neural Network Model for Dialogue Act Classification},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {524--529},
  abstract  = {We propose a novel generative neural network architecture for Dialogue Act
	classification. Building upon the Recurrent Neural Network framework, our model
	incorporates a novel attentional technique and a label to label connection for
	sequence learning, akin to Hidden Markov Models. The experiments show that both
	of these innovations lead  our model to outperform strong baselines for
	dialogue act classification on MapTask and Switchboard corpora. We further
	empirically analyse the effectiveness of each of the new innovations.},
  url       = {http://aclweb.org/anthology/P17-2083}
}

@InProceedings{teneva-cheng:2017:Short,
  author    = {Teneva, Nedelina  and  Cheng, Weiwei},
  title     = {Salience Rank: Efficient Keyphrase Extraction with Topic Modeling},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {530--535},
  abstract  = {Topical PageRank (TPR) uses latent topic distribution inferred by Latent
	Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from
	documents. The ranking procedure consists of running PageRank K times, where K
	is the number of topics used in the LDA model. In this paper, we propose a
	modification of TPR, called Salience Rank. Salience Rank only needs to run
	PageRank once and extracts comparable or better keyphrases on benchmark
	datasets. In addition to quality and efficiency benefit, our method has the
	flexibility to extract keyphrases with varying tradeoffs between topic
	specificity and corpus specificity.},
  url       = {http://aclweb.org/anthology/P17-2084}
}

@InProceedings{lin-lin-ji:2017:Short,
  author    = {Lin, Ying  and  Lin, Chin-Yew  and  Ji, Heng},
  title     = {List-only Entity Linking},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {536--541},
  abstract  = {Traditional Entity Linking (EL) technologies rely on rich structures and
	properties in the target knowledge base (KB). However, in many applications,
	the KB may be as simple and sparse as lists of names of the same type (e.g.,
	lists of products). We call it as List-only Entity Linking problem. 
	Fortunately, some mentions may have more cues for linking, which can be used as
	seed mentions to bridge other mentions and the uninformative entities. 
	In this work, we select most linkable mentions as seed mentions and
	disambiguate other mentions by comparing them with the seed mentions rather
	than directly with the entities.
	Our experiments on linking mentions to seven automatically mined lists show
	promising results and demonstrate the effectiveness of our approach.},
  url       = {http://aclweb.org/anthology/P17-2085}
}

@InProceedings{chen-strapparava-nastase:2017:Short,
  author    = {Chen, Lingzhen  and  Strapparava, Carlo  and  Nastase, Vivi},
  title     = {Improving Native Language Identification by Using Spelling Errors},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {542--546},
  abstract  = {In this paper, we explore spelling errors as a source of information for
	detecting the native language of a writer, a previously under-explored area.
	 We note that character n-grams from misspelled words are very indicative of
	the native language of the author. In combination with other lexical features,
	spelling error features lead to 1.2% improvement in accuracy on classifying
	texts in the TOEFL11 corpus by the author's native language, compared to
	systems participating in the NLI shared task.},
  url       = {http://aclweb.org/anthology/P17-2086}
}

@InProceedings{jamshidlou-johnson:2017:Short,
  author    = {Jamshid Lou, Paria  and  Johnson, Mark},
  title     = {Disfluency Detection using a Noisy Channel Model and a Deep Neural Language Model},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {547--553},
  abstract  = {This paper presents a model for disfluency detection in spontaneous speech
	transcripts called LSTM Noisy Channel Model. The model uses a Noisy Channel
	Model (NCM) to generate n-best candidate disfluency analyses and a Long
	Short-Term Memory (LSTM) language model to score the underlying fluent
	sentences of each analysis. The LSTM language model scores, along with other
	features, are used in a MaxEnt reranker to identify the most plausible
	analysis. We show that using an LSTM language model in the reranking process of
	noisy channel disfluency model improves the state-of-the-art in disfluency
	detection.},
  url       = {http://aclweb.org/anthology/P17-2087}
}

@InProceedings{hayashi-shimbo:2017:Short,
  author    = {Hayashi, Katsuhiko  and  Shimbo, Masashi},
  title     = {On the Equivalence of Holographic and Complex Embeddings for Link Prediction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {554--559},
  abstract  = {We show the equivalence of two state-of-the-art models for 
	link prediction/knowledge graph completion:
	Nickel et al's holographic embeddings and Trouillon et al.'s complex
	embeddings.
	We first consider a spectral version of the holographic embeddings,
	exploiting the frequency domain in the Fourier transform for efficient
	computation.
	The analysis of the resulting model reveals that it can be viewed as
	an instance of the complex embeddings
	with a certain constraint imposed on the initial vectors upon training.
	Conversely, any set of complex embeddings can be converted to a set of
	equivalent holographic embeddings.},
  url       = {http://aclweb.org/anthology/P17-2088}
}

@InProceedings{wang-EtAl:2017:Short3,
  author    = {Wang, Rui  and  Finch, Andrew  and  Utiyama, Masao  and  Sumita, Eiichiro},
  title     = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {560--566},
  abstract  = {Although new corpora are becoming increasingly available for machine
	translation, only those that belong to the same or similar domains are
	typically able to improve translation performance. Recently Neural Machine
	Translation (NMT) has become prominent in the field. However, most of the
	existing domain adaptation methods only focus on phrase-based machine
	translation. In this paper, we exploit the NMT's internal embedding of the
	source sentence and use the sentence embedding similarity to select the
	sentences which are close to in-domain data. The empirical adaptation results
	on the IWSLT English-French and NIST Chinese-English tasks show that the
	proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU
	points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU
	points.},
  url       = {http://aclweb.org/anthology/P17-2089}
}

@InProceedings{fadaee-bisazza-monz:2017:Short2,
  author    = {Fadaee, Marzieh  and  Bisazza, Arianna  and  Monz, Christof},
  title     = {Data Augmentation for Low-Resource Neural Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {567--573},
  abstract  = {The quality of a Neural Machine Translation system depends substantially on the
	availability of sizable parallel corpora.
	For low-resource language pairs this is not the case, resulting in poor
	translation quality. 
	Inspired by work in computer vision, we propose a novel data augmentation
	approach that targets low-frequency words by generating new sentence pairs
	containing rare words in new, synthetically created contexts.
	Experimental results on simulated low-resource settings show that our method
	improves translation quality by up to 2.9 BLEU points over the baseline and up
	to 3.2 BLEU over back-translation.},
  url       = {http://aclweb.org/anthology/P17-2090}
}

@InProceedings{shi-knight:2017:Short,
  author    = {Shi, Xing  and  Knight, Kevin},
  title     = {Speeding Up Neural Machine Translation Decoding by Shrinking Run-time Vocabulary},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {574--579},
  abstract  = {We speed up Neural Machine Translation (NMT) decoding by shrinking run-time
	target vocabulary. We experiment with two shrinking approaches: Locality
	Sensitive Hashing (LSH) and word alignments. Using the latter method, we get a
	2x overall speed-up over a highly-optimized GPU implementation, without hurting
	BLEU. On certain low-resource language pairs, the same methods improve BLEU by
	0.5 points. We also report a negative result for LSH on GPUs, due to relatively
	large overhead, though it was successful on CPUs. Compared with Locality
	Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly,
	orthogonal to existing speedup methods and more robust across language pairs.},
  url       = {http://aclweb.org/anthology/P17-2091}
}

@InProceedings{zhou-EtAl:2017:Short2,
  author    = {Zhou, Hao  and  Tu, Zhaopeng  and  Huang, Shujian  and  Liu, Xiaohua  and  Li, Hang  and  Chen, Jiajun},
  title     = {Chunk-Based Bi-Scale Decoder for Neural Machine Translation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {580--586},
  abstract  = {In typical neural machine translation~(NMT), the decoder generates a sentence
	word by word, packing all linguistic granularities in the same time-scale of
	RNN. In this paper, we propose a new type of decoder for NMT, which splits the
	decode state into two parts and updates them in two different time-scales.
	Specifically, we first predict a chunk time-scale state for phrasal modeling,
	on top of which multiple word time-scale states are generated.
	In this way, the target sentence is translated hierarchically from chunks to
	words, with information in different granularities being leveraged.
	Experiments show that our proposed model significantly improves the translation
	performance over the state-of-the-art NMT model.},
  url       = {http://aclweb.org/anthology/P17-2092}
}

@InProceedings{fang-cohn:2017:Short,
  author    = {Fang, Meng  and  Cohn, Trevor},
  title     = {Model Transfer for Tagging Low-resource Languages using a Bilingual Dictionary},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {587--593},
  abstract  = {Cross-lingual model transfer is a compelling and popular method for predicting
	annotations in a low-resource language, whereby parallel corpora provide a
	bridge to a high-resource language, and its associated annotated corpora.
	However, parallel data is not readily available for many languages, limiting
	the applicability of these approaches. We address these drawbacks in our
	framework which takes advantage of cross-lingual word embeddings trained solely
	on a high coverage dictionary. We propose a novel neural network model for
	joint training from both sources of data based on cross-lingual word
	embeddings, and show substantial empirical improvements over baseline
	techniques. We also propose several active learning heuristics, which result in
	improvements over competitive benchmark methods.},
  url       = {http://aclweb.org/anthology/P17-2093}
}

@InProceedings{dellibovi-EtAl:2017:Short,
  author    = {Delli Bovi, Claudio  and  Camacho-Collados, Jose  and  Raganato, Alessandro  and  Navigli, Roberto},
  title     = {EuroSense: Automatic Harvesting of Multilingual Sense Annotations from Parallel Text},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {594--600},
  abstract  = {Parallel corpora are widely used in a variety of Natural Language Processing
	tasks, from Machine Translation to cross-lingual Word Sense Disambiguation,
	where parallel sentences can be exploited to automatically generate
	high-quality sense annotations on a large scale. In this paper we present
	EuroSense, a multilingual sense-annotated resource based on the joint
	disambiguation of the Europarl parallel corpus, with almost 123 million sense
	annotations for over 155 thousand distinct concepts and entities from a
	language-independent unified sense inventory. We evaluate the quality of our
	sense annotations intrinsically and extrinsically, showing their effectiveness
	as training data for Word Sense Disambiguation.},
  url       = {http://aclweb.org/anthology/P17-2094}
}

@InProceedings{sajjad-EtAl:2017:Short,
  author    = {Sajjad, Hassan  and  Dalvi, Fahim  and  Durrani, Nadir  and  Abdelali, Ahmed  and  Belinkov, Yonatan  and  Vogel, Stephan},
  title     = {Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {601--607},
  abstract  = {Word segmentation plays a pivotal role in improving any Arabic NLP application.
	Therefore, a lot of research has been spent in improving its accuracy.
	Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect
	dependent. We explore three language-independent alternatives to morphological
	segmentation us- ing: i) data-driven sub-word units, ii) characters as a unit
	of learning, and iii) word embeddings learned using a character CNN
	(Convolution Neural Network). On the tasks of Machine Translation and POS
	tagging, we found these methods to achieve close to, and occasionally surpass
	state-of-the-art performance. In our analysis, we show that a neural machine
	translation system is sensitive to the ratio of source and target tokens, and a
	ratio close to 1 or greater, gives optimal performance.},
  url       = {http://aclweb.org/anthology/P17-2095}
}

@InProceedings{cai-EtAl:2017:Short,
  author    = {Cai, Deng  and  Zhao, Hai  and  Zhang, Zhisong  and  Xin, Yuan  and  Wu, Yongjian  and  Huang, Feiyue},
  title     = {Fast and Accurate Neural Word Segmentation for Chinese},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {608--615},
  abstract  = {Neural models with minimal feature engineering have achieved competitive
	performance against traditional methods for the task of Chinese word
	segmentation. However, both training and working procedures of the current
	neural models are computationally inefficient. In this paper, we propose a
	greedy neural word segmenter with balanced word and character embedding inputs
	to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable
	of performing segmentation much faster and even more accurate than
	state-of-the-art neural models on Chinese benchmark datasets.},
  url       = {http://aclweb.org/anthology/P17-2096}
}

@InProceedings{cai-tu-gimpel:2017:Short,
  author    = {Cai, Zheng  and  Tu, Lifu  and  Gimpel, Kevin},
  title     = {Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {616--622},
  abstract  = {We consider the ROC story cloze task (Mostafazadeh et al., 2016) and present
	several findings. We develop a model that uses hierarchical recurrent networks
	with attention to encode the sentences in the story and score candidate
	endings. By discarding the large training set and only training on the
	validation set, we achieve an accuracy of 74.7%. Even when we discard the story
	plots (sentences before the ending) and only train to choose the better of two
	endings, we can still reach 72.5%. We then analyze this “ending-only” task
	setting. We estimate human accuracy to be 78% and find several types of clues
	that lead to this high accuracy, including those related to sentiment,
	negation, and general ending likelihood regardless of the story context.},
  url       = {http://aclweb.org/anthology/P17-2097}
}

@InProceedings{herzig-berant:2017:Short,
  author    = {Herzig, Jonathan  and  Berant, Jonathan},
  title     = {Neural Semantic Parsing over Multiple Knowledge-bases},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {623--628},
  abstract  = {A fundamental challenge in developing semantic parsers is the paucity of strong
	supervision in the form of language utterances annotated with logical form. In
	this paper, we propose to exploit structural regularities in language in
	different domains, and train semantic parsers over multiple knowledge-bases
	(KBs), while sharing information across datasets. We find that we can
	substantially improve parsing accuracy by training a single
	sequence-to-sequence model over multiple KBs, when providing an encoding of the
	domain at decoding time. Our model achieves state-of-the-art performance on the
	Overnight dataset (containing eight domains), improves performance over a
	single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the
	number of model parameters.},
  url       = {http://aclweb.org/anthology/P17-2098}
}

@InProceedings{mu-bhat-viswanath:2017:Short,
  author    = {Mu, Jiaqi  and  Bhat, Suma  and  Viswanath, Pramod},
  title     = {Representing Sentences as Low-Rank Subspaces},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {629--634},
  abstract  = {Sentences are important semantic units of natural language. A generic,
	distributional representation of sentences that can capture the latent
	semantics is beneficial to multiple downstream applications. We observe a
	simple geometry of sentences -- the word representations of a given sentence
	(on average 10.23 words in all SemEval datasets with a standard deviation 4.84)
	roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this
	observation, we represent a sentence by the low-rank subspace spanned by its
	word vectors. Such an unsupervised representation is empirically validated via
	semantic textual similarity tasks on 19 different datasets, where it
	outperforms the sophisticated neural network models,  including skip-thought
	vectors, by 15% on average.},
  url       = {http://aclweb.org/anthology/P17-2099}
}

@InProceedings{ma-EtAl:2017:Short2,
  author    = {Ma, Shuming  and  Sun, Xu  and  Xu, Jingjing  and  Wang, Houfeng  and  Li, Wenjie  and  Su, Qi},
  title     = {Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {635--640},
  abstract  = {Current Chinese social media text summarization models are based on an
	encoder-decoder framework. Although its generated summaries are similar to
	source texts literally, they have low semantic relevance. In this work, our
	goal is to improve semantic relevance between source texts and summaries for
	Chinese social media summarization. We introduce a Semantic Relevance Based
	neural model to encourage high semantic similarity between texts and summaries.
	In our model, the source text is represented by a gated attention encoder,
	while the summary representation is produced by a decoder. Besides, the
	similarity score between the representations is maximized during training. Our
	experiments show that the proposed model outperforms baseline systems on a
	social media corpus.},
  url       = {http://aclweb.org/anthology/P17-2100}
}

@InProceedings{sanagavarapu-vempala-blanco:2017:Short,
  author    = {Sanagavarapu, Krishna Chaitanya  and  Vempala, Alakananda  and  Blanco, Eduardo},
  title     = {Determining Whether and When People Participate in the Events They Tweet About},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {641--646},
  abstract  = {This paper describes an approach to determine whether people participate in the
	events they tweet about. Specifically, we determine whether people are
	participants in events with respect to the tweet timestamp. We target all
	events expressed by verbs in tweets, including past, present and events that
	may occur in the future. We present new annotations using 1,096 event mentions,
	and experimental results showing that the task is challenging.},
  url       = {http://aclweb.org/anthology/P17-2101}
}

@InProceedings{volkova-EtAl:2017:Short,
  author    = {Volkova, Svitlana  and  Shaffer, Kyle  and  Jang, Jin Yea  and  Hodas, Nathan},
  title     = {Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {647--653},
  abstract  = {Pew research polls report 62 percent of U.S. adults get news on social media
	(Gottfried and Shearer, 2016). In a December poll, 64 percent of U.S. adults
	said that “made-up news” has caused a “great deal of confusion” about
	the facts of current events (Barthel et al., 2016). Fabricated stories in
	social media, ranging from deliberate propaganda to hoaxes and satire,
	contributes to this confusion in addition to having serious effects on global
	stability.
	In this work we build predictive models to classify 130 thousand news posts as
	suspicious or verified, and predict four sub-types of suspicious news --
	satire, hoaxes, clickbait and propaganda. We show that neural network models
	trained on tweet content and social network interactions outperform lexical
	models. Unlike previous work on deception detection, we find that adding syntax
	and grammar features to our models does not improve performance. Incorporating
	linguistic features improves classification results, however, social
	interaction features are most in- formative for finer-grained separation be-
	tween four types of suspicious news posts.},
  url       = {http://aclweb.org/anthology/P17-2102}
}

@InProceedings{son-EtAl:2017:Short,
  author    = {Son, Youngseo  and  Buffone, Anneke  and  Raso, Joe  and  Larche, Allegra  and  Janocko, Anthony  and  Zembroski, Kevin  and  Schwartz, H. Andrew  and  Ungar, Lyle},
  title     = {Recognizing Counterfactual Thinking in Social Media Texts},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {654--658},
  abstract  = {Counterfactual statements, describing events that did not occur and their
	consequents, have been studied in areas including problem-solving, affect
	management, and behavior regulation. People with more counterfactual thinking
	tend to perceive life events as more personally meaningful. Nevertheless,
	counterfactuals have not been studied in computational linguistics. We create a
	counterfactual tweet dataset and explore approaches for detecting
	counterfactuals using rule-based and supervised statistical approaches. A
	combined rule-based and statistical approach yielded the best results (F1 =
	0.77) outperforming either approach used alone.},
  url       = {http://aclweb.org/anthology/P17-2103}
}

@InProceedings{hasanuzzaman-EtAl:2017:Short,
  author    = {Hasanuzzaman, Mohammed  and  Kamila, Sabyasachi  and  Kaur, Mandeep  and  Saha, Sriparna  and  Ekbal, Asif},
  title     = {Temporal Orientation of Tweets for Predicting Income of Users},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {659--665},
  abstract  = {Automatically estimating a user's socio-economic profile from their language
	use in social media can significantly help social science research and various
	downstream applications ranging from business to politics. The current paper
	presents the first study where user cognitive structure is used to build a
	predictive model of income. In particular, we first develop a classifier using
	a weakly supervised learning framework to automatically time-tag tweets as
	past, present, or future. We quantify a user's overall temporal orientation
	based on their distribution of tweets, and use it to build a predictive model
	of income. Our analysis uncovers a correlation between future temporal
	orientation and income. Finally, we measure the predictive power of future
	temporal orientation on income by performing regression.},
  url       = {http://aclweb.org/anthology/P17-2104}
}

@InProceedings{toleu-tolegen-makazhanov:2017:Short,
  author    = {Toleu, Alymzhan  and  Tolegen, Gulmira  and  Makazhanov, Aibek},
  title     = {Character-Aware Neural Morphological Disambiguation},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {666--671},
  abstract  = {We develop a language-independent, deep learning-based approach to the task of
	morphological disambiguation.
	Guided by the intuition that the correct analysis should be ``most similar'' to
	the context, we propose dense representations for morphological analyses and
	surface context and a simple yet effective way of combining the two to perform
	disambiguation.
	Our approach improves on the language-dependent state of the art for two
	agglutinative languages (Turkish and Kazakh) and can be potentially applied to
	other morphologically complex languages.},
  url       = {http://aclweb.org/anthology/P17-2105}
}

@InProceedings{yu-vu:2017:Short,
  author    = {Yu, Xiang  and  Vu, Ngoc Thang},
  title     = {Character Composition Model with Convolutional Neural Networks for Dependency Parsing on Morphologically Rich Languages},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {672--678},
  abstract  = {We present a transition-based dependency parser that uses a convolutional
	neural network to compose word representations from characters. The character
	composition model shows great improvement over the word-lookup model,
	especially
	for parsing agglutinative languages. These improvements are even better than
	using pre-trained word
	embeddings from extra data. On the SPMRL data sets, our system outperforms the
	previous best greedy parser (Ballesteros et. al, 2015) by a margin of 3% on
	average.},
  url       = {http://aclweb.org/anthology/P17-2106}
}

@InProceedings{agic-schluter:2017:Short,
  author    = {Agi\'{c}, \v{Z}eljko  and  Schluter, Natalie},
  title     = {How (not) to train a dependency parser: The curious case of jackknifing part-of-speech taggers},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {679--684},
  abstract  = {In dependency parsing, jackknifing taggers is indiscriminately used as a simple
	adaptation strategy. Here, we empirically evaluate when and how (not) to use
	jackknifing in parsing. On 26 languages, we reveal a preference that conflicts
	with, and surpasses the ubiquitous ten-folding. We show no clear benefits of
	tagging the training data in cross-lingual parsing.},
  url       = {http://aclweb.org/anthology/P17-2107}
}

